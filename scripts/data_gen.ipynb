{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15281c16",
   "metadata": {},
   "source": [
    "To gather user comments and other relevant information, we will employ the Python Reddit API Wrapper (PRAW) which builds upon the publically available reddit API in a user-friendly manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d348eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import time\n",
    "import pandas as pd\n",
    "from prawcore.exceptions import TooManyRequests\n",
    "\n",
    "# initialize login-info\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"SgqCbXgBegHP1bRZ2qc7FQ\",\n",
    "    client_secret=\"OrSeWWIvWoOJl7WVniFKDlMSkvW-rQ\",\n",
    "    user_agent=\"macos:Sentiment_Analysis:v1 (by u/Minimum-Tadpole-5000)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b685de1",
   "metadata": {},
   "source": [
    "The previous block initializes a read-only instance of reddit that can be used to access public subbreddits of interest for submissions, comments, redditors, etc. To perform meaningful analysis, we are motivated to create a CSV file of the data. The following function definition does exactly that, manipulating the CommentForest object and gathering all comments through a breadth-first-search procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f16ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_as_csv(subreddit: str, num_submissions: int, sort_method: str, delay: float = 2.0):\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "    comments = []\n",
    "    sort_type = getattr(sub, sort_method)\n",
    "    for i, submission in enumerate(sort_type(limit=num_submissions)): \n",
    "        try:\n",
    "            submission.comments.replace_more(limit=None) # flatten nested comment structure\n",
    "            for comment in submission.comments.list():\n",
    "                comments.append({\n",
    "                    \"id\": comment.id,\n",
    "                    \"author\": str(comment.author),\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"comment_karma\": getattr(comment.author, \"comment_karma\", None),\n",
    "                    \"created_utc\": comment.created_utc\n",
    "                })\n",
    "            time.sleep(delay)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit hit. Resting for a while...\")\n",
    "            time.sleep(e.sleep_time if hasattr(e, 'sleep_time') else 60)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing submission {i+1}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(comments)\n",
    "    df['subreddit'] = subreddit\n",
    "    df.to_csv(f\"../data/{subreddit}-{sort_method}-{num_submissions}.csv\", index=False) # save to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a2dbb0",
   "metadata": {},
   "source": [
    "We can now focus our data collection on a certain subject across subreddits to analyze sentiment. For example, we can gather the comments from several of the most recent hot posts of NBA subreddits for various teams. Recall that gathering this data relies on the public reddit API, which can only handle so many requests at a time. It is important to respect the network and avoid overloading it with requests by spacing out calls. The block below defines a list of NBA subreddits and fetches the comments from 10 hot posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_subs = [\"AtlantaHawks\", \"bostonceltics\", \"GoNets\", \"CharlotteHornets\", \"chicagobulls\",\n",
    "            \"clevelandcavs\", \"DetroitPistons\", \"pacers\", \"heat\", \"MkeBucks\", \"NYKnicks\",\n",
    "            \"OrlandoMagic\", \"sixers\", \"torontoraptors\", \"washingtonwizards\", \"Mavericks\",\n",
    "            \"denvernuggets\", \"warriors\", \"rockets\", \"LAClippers\", \"lakers\", \"memphisgrizzlies\",\n",
    "            \"timberwolves\", \"NOLAPelicans\", \"Thunder\", \"suns\", \"ripcity\", \"kings\", \"NBASpurs\",\n",
    "            \"UtahJazz\"]\n",
    "\n",
    "for subreddit in nba_subs:\n",
    "    get_comments_as_csv(subreddit, 10, \"hot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966f66f",
   "metadata": {},
   "source": [
    "After gathering the individual csv files, we can combine them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64ff3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# grab all file paths\n",
    "files = glob.glob(\"../data/*.csv\")\n",
    "\n",
    "combined_data = pd.concat([pd.read_csv(f) for f in files])\n",
    "\n",
    "combined_data.to_csv(\"../data/comments.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
